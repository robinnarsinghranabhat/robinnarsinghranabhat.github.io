---
title:  "Policy Gradients in Machine Learning"
date:   2020-09-19 23:35:38 +0545
classes : wide
categories:
  - Data-Science
  - Testing
excerpt: A brief Intro to Policy Gradients algorithm for Reinforcement Learning

header:
  og_image: /assets/images/sentence_embedding_image.jpg
  teaser: /assets/images/sentence_embedding_image.jpg
---


### THE SLIDES FOR THIS POST ARE
https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf

### Lecture Notes of this post
http://web.stanford.edu/class/cs234/CS234Win2019/slides/lnotes8.pdf


Previously in DQN, Network converted state (vector data, pixels , frame...) into state value. ( We approximated value / action value based on theta of neural net)
We trained the network on which action to choose from among different action-values.
 For early exploration, when network weights are random, we used epsilon greedy. As for evaluation, we used argmax of action. As for
 how DQN learn, basically, it's through some tiny occasional reward signal as a guiding light.

Now, we are more straight forward. Policy is probability distribution over actions, coonditioned on state and parameters.

Our network is optimized to generate set of actions conditioned on weights of network and new states that get generated as we take new actions,
to maximize reward over that trajectory / Time period / Episode .. Based on how problem is set.

Basically .. That's it . unlike DQN, output of network is not how good taking an action from an state is , but, just, what action to take, given current state and network weights, during a course of some trajectory.
And remember, Network learns to maximize reward over an trajectory. and not just on single action output.


##### Paste image from slide1 for Davil silver's policy gradient video.

## Motivation
IN some situates,
Imagine playing an Atari Game Breakout, and given pixel values of an state, it doesn't make much sense, to figure out how good that state is.
But, represent fact that, if ball is here, take this action is just easier.

- Better Convergence properties .. , Value based often oscillate , dramatic swings
- Good for high dimensinal / Contionoues action spaces
- Learn stochastic policies .. Since network is trained to take series of actions over an trajectory ( an episode, some thousand time steps .. how you define it ),
  it may take multiple trajectories .. since, choos- Value function
ing action is based on probability distribution of actions.
- Scale to large complicated MDP's. Even with DQN, assigining values to states is a complicated job, for real world problems.


## Policy Gradient when action space is discrete ,

Neural network with softmax output .. And we sample from that ..
Note, since output is sampled based on probability of different actions, taking gradient is a bit different here.


## Policy Gradient when action space is scalar , but contionous .. That pendulum game, where output ranges between 0 to 1



## Policy Gradient when action space is high dimensional .. Well well well


( DESCRIBING SLIDE FOR SCISSOR PAPER ROCK) SLIDE -> 6

Why stochastic policy ?? Why not deterministic policy to maximize the reward through a path ??

Conside **Scissor , Papers , Rock** . Here, deterministic strategy is easily exploited. Optimal policy is to play uniform random. Inspired by Nash Equilibrium from Game Theory.


#### IN The SLIDE 7 , 8 , 9 , WE discuss STATE ALIASING  ( Multiple states with same features )
## ( STATE ALIASING : Multiple states in an MDP, whose feature is same i.e Gray regions in Fugure of slide 7 ) ###

(DESCRIBING ALIASED GRID WORLD, with VALUE METHOD ) SLIDE 7 -> Value function will assign same values to those Gray regions.
So, That mean, With DQN algorithms, that assign same values to those state-action pair in Gray regions, it's clear that, we will choose *deterministic action with high probability as epsilon is low and during testing of algorithm, som implementation dont use epsilon. It's used in start only for exploration.*

So, From figure, agent will get stuck oscillating , is in figure, until by virtue of epsilon, get out of it after a long time.

(DESCRIBING ALIASED GRID WORLD, with VALUE METHOD)  SLIDE 8

Now, Since probability is not deterministic, agent has 50-50 percent change of going E or W from first gray region.
Simliar is the case for second gray region as well.

       PI( state_feature = ( Wall at N and S  ) , action = Move E ) = 0.5
       PI( state_feature = ( Wall at N and S  ) , action = Move W ) = 0.5

In deep RL : PI represents the neural network whose input is state_feature : and discrete action is defined as softmax output. ( How about continous action space , contionus high dimensional action spaces .. We will go there . . Just wait )


### CONCLUSION STATE ALIASING ###########
#########################################

### STARTING : POLICY OBJECTIVE FUNCTION .. How do we optimize it really ?

DESCRIBING SLIDE 9 --- Policy Optimization functions
