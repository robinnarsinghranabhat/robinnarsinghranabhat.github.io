---
title:  "Policy Gradients in Machine Learning"
date:   2020-09-19 23:35:38 +0545
classes : wide
categories:
  - Data-Science
  - Testing
excerpt: A brief Intro to Policy Gradients algorithm for Reinforcement Learning

header:
  og_image: /assets/images/sentence_embedding_image.jpg
  teaser: /assets/images/sentence_embedding_image.jpg
---


### THE SLIDES FOR THIS POST ARE
https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf

Previously in DQN, Network converted state (vector data, pixels , frame...) into state value. ( We approximated value / action value based on theta of neural net)
We trained the network on which action to choose from among different action-values.
 For early exploration, when network weights are random, we used epsilon greedy. As for evaluation, we used argmax of action. As for
 how DQN learn, basically, it's through some tiny occasional reward signal as a guiding light.

Now, we are more straight forward. Policy is probability distribution over actions, coonditioned on state and parameters.

Our network is optimized to generate set of actions conditioned on weights of network and new states that get generated as we take new actions,
to maximize reward over that trajectory / Time period / Episode .. Based on how problem is set.

Basically .. That's it . unlike DQN, output of network is not how good taking an action from an state is , but, just, what action to take, given current state and network weights, during a course of some trajectory.
And remember, Network learns to maximize reward over an trajectory. and not just on single action output.


##### Paste image from slide1 for Davil silver's policy gradient video.

## Motivation
IN some situates,
Imagine playing an Atari Game Breakout, and given pixel values of an state, it doesn't make much sense, to figure out how good that state is.
But, represent fact that, if ball is here, take this action is just easier.

- Better Convergence properties .. , Value based often oscillate , dramatic swings
- Good for high dimensinal / Contionoues action spaces
- Learn stochastic policies .. Since network is trained to take series of actions over an trajectory ( an episode, some thousand time steps .. how you define it ),
  it may take multiple trajectories .. since, choosing action is based on probability distribution of actions.



- Scale to large complicated MDP's. Even with DQN, assigining values to states is a complicated job, for real world problems.
- Value function


## Policy Gradient when action space is discrete ,

Neural network with softmax output .. And we sample from that ..
Note, since output is sampled based on probability of different actions, taking gradient is a bit different here.


## Policy Gradient when action space is scalar , but contionous .. That pendulum game, where output ranges between 0 to 1



## Policy Gradient when action space is high dimensional .. Well well well








## Why test ?

- Is code working as expected ( confidence that results are indeed correct)
- Make experimental changes rapidly without fearing to break the code
- Other people more confident in our code

-Helps catch bugs
-Understand the code for new users
-Help code development and refactoring

- Code works not just in one computer, but across all the other users computer

```

Demo :

```python
## In this sample code, we see a basic testing
## We test our function ***capitalize_reverse***
## The function is supposed to capitalize text and reverse it, while ignoring numbers

## We make a separate file test_demo.py and run the code below as : **pytest test_demo.py**

def capitalize_reverse(text):
    return text[::-1].upper()

def test_capitalize_reserse():
  assert capitalize_reverse('age') == 'EGA'
  assert capitalize_reverse('1243') == '3421'

def test_capitalize_reserse():
    assert capitalize_reverse('age') == 'EGA'
    assert capitalize_reverse('1243') == '3421'

```

We get following output of successful testing :

![](/assets/images/test_1.png){:.align-center}


## Why not test ?

- Time taking

## How many testing variants to add ?

Imagine when they mass produced the first car (your first piece of code that works). How much crashes did it take to perfect the car design ? after one crash, they decided to use bumper, and after another, they decided to put air bag and so on .. Similarly, as code keeps breaking ( someone uses string in addition function ), we continously modify code and add new tests along the way.


## How to assert code is actually correct ?

- Manual sanity checks in head ( Try ways to break the code)
- Defensive Programming ( Tons of if else , exception handling , assertions )
- Tests



## Pytest Basics


## Fixtures to prevent rerunning time straining pieces of code .. loading a function

## Mocks .. Creating a fake response ..
